{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIAL IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\servi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "from imutils import paths\n",
    "import shutil\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder, Kinetics\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset,ConcatDataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD THE FACEFORENSIC'S DATASET FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"kaggle\" non ï¿½ riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"stefanoservillo\" # username from the json file\n",
    "os.environ['KAGGLE_KEY'] = \"d820211db8d6f4de9f2656d1eb4c2c38\" # key to connect with Kaggle API\n",
    "\n",
    "!kaggle datasets download -d stefanoservillo/deepfake #kaggle command to download the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip deepfake.zip #we have to unzip all the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDE DATASET IN TRAIN AND VALIDATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the video repository\n",
    "DATASET_PATH = \"/content/deepfake\"\n",
    "\n",
    "# path for the train and validation set\n",
    "TRAIN = \"train\"\n",
    "VALIDATION = \"val\"\n",
    "\n",
    "# split of validation set\n",
    "VAL_SPLIT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_videos(videoPaths, folder):\n",
    "\t\n",
    "    # check if the destination folder exists and if not create it\n",
    "\tif not os.path.exists(folder):\n",
    "\t\tos.makedirs(folder)\n",
    "\t\n",
    "    # loop over the image paths\n",
    "\tfor path in videoPaths:\n",
    "\t\t\n",
    "        # grab image name and its label from the path and create\n",
    "\t\t# a placeholder corresponding to the separate label folder\n",
    "\t\tvideoName = path.split(os.path.sep)[-1]\n",
    "\t\tlabel = path.split(os.path.sep)[-2]\n",
    "\t\tlabelFolder = os.path.join(folder, label)\n",
    "\t\t\n",
    "        # check to see if the label folder exists and if not create it\n",
    "\t\tif not os.path.exists(labelFolder):\n",
    "\t\t\tos.makedirs(labelFolder)\n",
    "\t\t\n",
    "        # construct the destination image path and copy the current\n",
    "\t\t# image to it\n",
    "\t\tdestination = os.path.join(labelFolder, videoName)\n",
    "\t\tshutil.copy(path, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading video paths...\")\n",
    "videoAlteredPaths = list(paths.list_files(DATASET_PATH+'/altered'))\n",
    "np.random.shuffle(videoAlteredPaths)\n",
    "\n",
    "# generate training and validation paths\n",
    "valAlteredPathsLen = int(len(videoAlteredPaths) * VAL_SPLIT)\n",
    "trainAlteredPathsLen = len(videoAlteredPaths) - valAlteredPathsLen\n",
    "trainPaths = videoAlteredPaths[:trainAlteredPathsLen]\n",
    "valPaths = videoAlteredPaths[trainAlteredPathsLen:]\n",
    "\n",
    "# copy the training and validation images to their respective\n",
    "# directories\n",
    "print(\"[INFO] copying training and validation videos...\")\n",
    "copy_videos(trainPaths, TRAIN)\n",
    "copy_videos(valPaths, VALIDATION)\n",
    "\n",
    "# load all the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading video paths...\")\n",
    "videoOriginalPaths = list(paths.list_files(DATASET_PATH+'/original'))\n",
    "np.random.shuffle(videoOriginalPaths)\n",
    "\n",
    "# generate training and validation paths\n",
    "valOriginPathsLen = int(len(videoOriginalPaths) * VAL_SPLIT)\n",
    "trainOriginPathsLen = len(videoOriginalPaths) - valOriginPathsLen\n",
    "trainPaths = videoOriginalPaths[:trainOriginPathsLen]\n",
    "valPaths = videoOriginalPaths[trainOriginPathsLen:]\n",
    "\n",
    "# copy the training and validation images to their respective\n",
    "# directories\n",
    "print(\"[INFO] copying training and validation videos...\")\n",
    "copy_videos(trainPaths, TRAIN)\n",
    "copy_videos(valPaths, VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DIRECTORY TO INSERT THE OPTICAL FLOW IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/valOpticalDataset\n",
    "!mkdir /content/valOpticalDataset/original\n",
    "!mkdir /content/valOpticalDataset/altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/trainOpticalDataset\n",
    "!mkdir /content/trainOpticalDataset/original\n",
    "!mkdir /content/trainOpticalDataset/altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECT FACE FROM EACH FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)  \n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    if (len(faces) == 0):\n",
    "        return None\n",
    "    \n",
    "    return gray[y:y + w, x:x + h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATE OPTICAL FLOW AND SAVE THE IMAGE OF THE FRAME IN A DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(prvs, nxt, number, directory,flag,type1=None):\n",
    "    \n",
    "    # Creates an array filled with zero \n",
    "    # with the same dimensions of the frame\n",
    "    rgb_image = cv2.cvtColor(prvs, cv2.COLOR_GRAY2BGR)\n",
    "    hsv = np.zeros_like(rgb_image)\n",
    "    hsv[..., 1] = 255\n",
    "\n",
    "    # Compute the optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, nxt, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Magnitude and angle of the 2D vectors\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Sets image hue and value according to the optical flow direction\n",
    "    # and magnitude, then converts HSV to RGB (BGR) color representation\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    if flag:\n",
    "      plt.imsave('/content/{}OpticalDataset/{}{}.png'.format(type1,directory,number),bgr)\n",
    "    else:\n",
    "      plt.imsave('/content/provaOptical/video/{}{}.png'.format(directory,number),bgr)\n",
    "    \n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAKE FRAMES FROM VIDEO, FIND THE FACE AND CALCULATE OPTICAL FLOW ON IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames(path,type1):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    ret, old_frame = video.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        return\n",
    "    face1 = detect_face(old_frame)\n",
    "    path = path.split('/')[1]+'/'+path.split('/')[2][0:-4]\n",
    "\n",
    "    number = 0\n",
    "    while(number<20):\n",
    "        number +=1\n",
    "        ret, new_frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"The video is finished\")\n",
    "            break\n",
    "        face2 = detect_face(new_frame)\n",
    "\n",
    "        try:\n",
    "          face1 = cv2.resize(face1, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          face2 = cv2.resize(face2, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          optical_flow = compute_optical_flow(face1, face2, number,path,True,type1)\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "\n",
    "        old_frame = new_frame\n",
    "        face1 = face2\n",
    "    video.release()\n",
    "\n",
    "videoTrainAlteredPaths = list(paths.list_files('train/altered'))\n",
    "for i in videoTrainAlteredPaths:\n",
    "  frames(i,'train')\n",
    "videoTrainOriginalPaths = list(paths.list_files('train/original'))\n",
    "for i in videoTrainOriginalPaths:\n",
    "  frames(i,'train')\n",
    "videoValAlteredPaths = list(paths.list_files('val/altered'))\n",
    "for i in videoValAlteredPaths:\n",
    "  frames(i,'val')\n",
    "videoValOriginalPaths = list(paths.list_files('val/original'))\n",
    "for i in videoValOriginalPaths:\n",
    "  frames(i,'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORM THE NPARRAY IN TENSOR AND SELECT THE DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "BATCH_SIZE=32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GIVE THE CORRECT DIRECTORY TO THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=ImageFolder('/content/trainOpticalDataset',transform=transfrom)\n",
    "valDataset=ImageFolder('/content/valOpticalDataset',transform=transfrom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DIFFERENT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(trainDataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader=DataLoader(valDataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHOW IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimages(imgs,actual_lbls,pred_lbls=None):\n",
    "  total = 0\n",
    "  good = 0\n",
    "  fig = plt.figure(figsize=(21,12))\n",
    "\n",
    "  for i,img in enumerate(imgs):\n",
    "    total +=1\n",
    "    fig.add_subplot(4,8, i+1)\n",
    "    y=actual_lbls[i]\n",
    "    \n",
    "    if pred_lbls!=None:\n",
    "      y_pre=pred_lbls[i]\n",
    "      title=\"prediction: {0}\\nlabel:{1}\".format(trainDataset.classes[y_pre],trainDataset.classes[y])\n",
    "      if trainDataset.classes[y] == trainDataset.classes[y_pre]:\n",
    "        good +=1\n",
    "    else: \n",
    "      title=\"Label: {0}\".format(trainDataset.classes[y])\n",
    "\n",
    "    plt.title(title)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "  if pred_lbls != None:\n",
    "    print(\"Accuracy: \"+ str((good/total)*100)+ '%')\n",
    "  \n",
    "plt.show()\n",
    "\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "showimages(inputs,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loss_fn,dataloader,optimizer,epoch):\n",
    "  print('\\nEpoch : %d'%epoch)\n",
    "  \n",
    "  total_loss=0    \n",
    "  correct=0\n",
    "  total=0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for data in tqdm(dataloader):\n",
    "    \n",
    "    inputs,labels=data[0].to(device),data[1].to(device)\n",
    "    \n",
    "    outputs=model(inputs)\n",
    "    \n",
    "    loss=loss_fn(outputs,labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    _, predicted = outputs.max(1)\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "  loss=total_loss/len(dataloader)\n",
    "  accuracy=100.*correct/total\n",
    "  \n",
    "  accuracies['train'].append(accuracy)\n",
    "  losses['train'].append(loss)\n",
    "  print('Train Loss: %.3f | Accuracy: %.3f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,loss_fn,dataloader,epoch):\n",
    "  model.eval()\n",
    "\n",
    "  total_loss=0\n",
    "  correct=0\n",
    "  total=0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(dataloader):\n",
    "      images,labels=data[0].to(device),data[1].to(device)\n",
    "      \n",
    "      outputs=model(images)\n",
    "\n",
    "      loss= loss_fn(outputs,labels)\n",
    "      total_loss+=loss.item()\n",
    "      \n",
    "      _, predicted = outputs.max(1)\n",
    "      total += labels.size(0)\n",
    "      correct += predicted.eq(labels).sum().item()\n",
    "  \n",
    "  loss=total_loss/len(dataloader)\n",
    "  accuracy=100.*correct/total\n",
    "\n",
    "  losses['val'].append(loss)\n",
    "  accuracies['val'].append(accuracy)\n",
    "\n",
    "  print('Test Loss: %.3f | Accuracy: %.3f'%(loss,accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN AND TEST IN EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'train':[], 'val':[]}\n",
    "accuracies = {'train':[], 'val':[]}\n",
    "epochs=20\n",
    "for epoch in range(1,epochs+1): \n",
    "  train(model,loss_fn,train_loader,optimizer_ft,epoch)\n",
    "  test(model,loss_fn,val_loader,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(accuracies['train'], label='Training Accuracy')\n",
    "plt.plot(accuracies['val'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(losses['train'], label='Training Loss')\n",
    "plt.plot(losses['val'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(model,images,actual_label):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    inputs = images.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    showimages(images,actual_label,preds.cpu())\n",
    "\n",
    "images, classes = next(iter(val_loader))\n",
    "\n",
    "predict_images(model,images,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST A RANDOM VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d stefanoservillo/testtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip testtt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/provaOptical\n",
    "!mkdir /content/provaOptical/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames(path):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    ret, old_frame = video.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        return\n",
    "    face1 = detect_face(old_frame)\n",
    "    path = path.split('/')[3][0:-4]\n",
    "\n",
    "    number = 0\n",
    "    while(video.isOpened()):\n",
    "        number +=1\n",
    "        ret, new_frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"The video is finished\")\n",
    "            break\n",
    "        face2 = detect_face(new_frame)\n",
    "\n",
    "        try:\n",
    "          face1 = cv2.resize(face1, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          face2 = cv2.resize(face2, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          optical_flow = compute_optical_flow(face1, face2, number,path,False)\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "\n",
    "        old_frame = new_frame\n",
    "        face1 = face2\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames('/content/prova/004.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimages(imgs,pred_lbls):\n",
    "  total = 0\n",
    "  fake = 0\n",
    "  fig = plt.figure(figsize=(21,12))\n",
    "\n",
    "  for i,img in enumerate(imgs):\n",
    "    total +=1\n",
    "    fig.add_subplot(4,8, i+1)\n",
    "    \n",
    "    y_pre=pred_lbls[i]\n",
    "    title=\"prediction: {0}\".format(trainDataset.classes[y_pre])\n",
    "    if trainDataset.classes[y_pre] == 'altered':\n",
    "      fake +=1\n",
    "\n",
    "    plt.title(title)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "  if pred_lbls != None:\n",
    "    print(\"Video fake al \"+ str((fake/total)*100)+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=ImageFolder('/content/provaOptical',transform=transfrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader=DataLoader(Dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(model,images):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    inputs = images.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    showimages(images,preds.cpu())\n",
    "\n",
    "images, classes = next(iter(data_loader))\n",
    "\n",
    "predict_images(model,images)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d2d07ecf2d88860f2c138e56ba9f113ddbe5b2c99bcbdd2c585f89a3a040e56"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
