{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECT DEEPFAKES NETWORK\n",
    "\n",
    "First, we need to import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\servi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "from imutils import paths\n",
    "import shutil\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm #to show progressive meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've decided to use Faceforesics' dataset which contains real and fake videos.\n",
    "To download the dataset from kaggle you need to upload the .json file into the .kaggle directory or to use the function:\n",
    "\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"username from the json file\"\n",
    "\n",
    "os.environ['KAGGLE_KEY'] = \"key from the json file\"\n",
    "\n",
    "Below, we download the dataset from kaggle and we unzip the folder in a destination directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"kaggle\" non ï¿½ riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"stefanoservillo\"\n",
    "os.environ['KAGGLE_KEY'] = \"d820211db8d6f4de9f2656d1eb4c2c38\"\n",
    "\n",
    "!kaggle datasets download -d sorokin/faceforensics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /content/data\n",
    "\n",
    "!unzip faceforensics.zip -d /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "TRAINING AND VALIDATION SET\n",
    "----------------------------------------\n",
    "Then we divide the dataset in training and validation set. In order to do this, we define training and validation paths and the percentage of videos we want in our validation set. We decide to take only 400 videos from the dataset due to memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to the dataset\n",
    "DATASET_PATH = \"/content/data\"\n",
    "\n",
    "# specify the paths to our training and validation set\n",
    "TRAIN = \"train\"\n",
    "VALIDATION = \"val\"\n",
    "\n",
    "# split of validation set\n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have the copy_videos() function, which takes as input the list of paths (in DATASET_PATH) and a destination folder for our videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_videos(videoPaths, folder, Set):\n",
    "\n",
    "  if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "  \n",
    "  number = 0\n",
    "  max_videos = 0\n",
    "  \n",
    "  # 150 videos for training for each type\n",
    "  if folder=='train':\n",
    "    max_videos = 150\n",
    "  #50 videos for validation for each type\n",
    "  else:\n",
    "    max_videos = 50\n",
    "  \n",
    "  while(number<max_videos):\n",
    "\n",
    "    path = videoPaths[number]\n",
    "    # grab image name and its label from the path and create\n",
    "\t\t# a placeholder corresponding to the separate label folder\n",
    "    videoName = path.split(os.path.sep)[-1]\n",
    "    labelFolder = os.path.join(folder, Set)\n",
    "\t\t\n",
    "    # check to see if the label folder exists and if not create it\n",
    "    if not os.path.exists(labelFolder):\n",
    "      os.makedirs(labelFolder)\n",
    "\t\t\n",
    "    # construct the destination image path and copy the current\n",
    "\t\t# image to it\n",
    "    destination = os.path.join(labelFolder, videoName)\n",
    "    shutil.copy(path, destination)\n",
    "  \n",
    "    number+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the image paths and we randomly shaffle them. Then, we generate training and validation paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all altered image paths and randomly shuffle them\n",
    "print(\"[INFO] loading video paths...\")\n",
    "videoAlteredPaths = list(paths.list_files(DATASET_PATH+'/manipulated_sequences/Deepfakes/c23/videos'))\n",
    "np.random.shuffle(videoAlteredPaths)\n",
    "\n",
    "# generate altered training and validation paths\n",
    "valAlteredPathsLen = int(len(videoAlteredPaths) * VAL_SPLIT)\n",
    "trainAlteredPathsLen = len(videoAlteredPaths) - valAlteredPathsLen\n",
    "trainPaths = videoAlteredPaths[:trainAlteredPathsLen]\n",
    "valPaths = videoAlteredPaths[trainAlteredPathsLen:]\n",
    "\n",
    "# copy the altered training and validation images to their respective\n",
    "# directories\n",
    "print(\"[INFO] copying training and validation altered videos...\")\n",
    "copy_videos(trainPaths, TRAIN, \"altered\")\n",
    "copy_videos(valPaths, VALIDATION, \"altered\")\n",
    "\n",
    "# load all the original image paths and randomly shuffle them\n",
    "print(\"[INFO] loading video paths...\")\n",
    "videoOriginalPaths = list(paths.list_files(DATASET_PATH+'/original_sequences/youtube/c23/videos'))\n",
    "np.random.shuffle(videoOriginalPaths)\n",
    "\n",
    "# generate original training and validation paths\n",
    "valOriginPathsLen = int(len(videoOriginalPaths) * VAL_SPLIT)\n",
    "trainOriginPathsLen = len(videoOriginalPaths) - valOriginPathsLen\n",
    "trainPaths = videoOriginalPaths[:trainOriginPathsLen]\n",
    "valPaths = videoOriginalPaths[trainOriginPathsLen:]\n",
    "\n",
    "# copy the original training and validation images to their respective\n",
    "# directories\n",
    "print(\"[INFO] copying training and validation original videos...\")\n",
    "copy_videos(trainPaths, TRAIN, \"original\")\n",
    "copy_videos(valPaths, VALIDATION, \"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "OPTICAL FLOW EXTRACTION\n",
    "----------------------------------------\n",
    "Once we've copied our files in training and validation folders, we can preprocess the videos.\n",
    "First, we create some directories to store our optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opticalPath = \"/content/optical_flow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(opticalPath):\n",
    "  os.makedirs(opticalPath)\n",
    "  os.makedirs(opticalPath+\"training/original\")\n",
    "  os.makedirs(opticalPath+\"training/altered\")\n",
    "  os.makedirs(opticalPath+\"validation/original\")\n",
    "  os.makedirs(opticalPath+\"validation/altered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the optical flow we detect the faces for each frame and we crop them. Otherwise, the computation will be too slow and too heavy.\n",
    "\n",
    "Below, we present the function which is meant to detect the faces in a given frame. We decide to use a Haar feature-based cascade classifiers to do this. The function returns a crop of the image around the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)  \n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    if (len(faces) == 0):\n",
    "        return None\n",
    "    \n",
    "    return gray[y - 40 : y + w + 40, x - 40 : x + h + 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a way to compute the optical flow for training and validation set. We use the Flow Farneback method and we save the results in the created directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(prvs, nxt, number, name,flag,Set=None):\n",
    "    \n",
    "    # Creates an array filled with zero \n",
    "    # with the same dimensions of the frame\n",
    "    rgb_image = cv2.cvtColor(prvs, cv2.COLOR_GRAY2BGR)\n",
    "    hsv = np.zeros_like(rgb_image)\n",
    "    hsv[..., 1] = 255\n",
    "\n",
    "    # Compute the optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, nxt, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Magnitude and angle of the 2D vectors\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Sets image hue and value according to the optical flow direction\n",
    "    # and magnitude, then converts HSV to RGB (BGR) color representation\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    if flag:\n",
    "      plt.imsave(opticalPath+'{}/{}-{}.png'.format(Set,name,number),bgr)\n",
    "    else:\n",
    "      plt.imsave('/content/finalVideo/video/{}-{}.png'.format(name,number),bgr)\n",
    "    \n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the function where we extract the frame from the training/validation videos and where we compute the optical flow.\n",
    "Then, we call it. \n",
    "\n",
    "For both training and validation set we compute and save the optical flow of three random frames for each video. This because we can't extract every frame from each video or we will run out of memory. Moreover, taking three frames for each video, gives us very different data to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames(path,Set):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    ret, old_frame = video.read()\n",
    "    if not ret:\n",
    "        print('No frames!')\n",
    "        return\n",
    "    face1 = detect_face(old_frame)\n",
    "    # name of the video\n",
    "    path = path.split('/')[1]+'/'+path.split('/')[2][0:-4]\n",
    "\n",
    "    number = 0\n",
    "    while(number<3):\n",
    "        \n",
    "        ret, new_frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"The video is finished\")\n",
    "            break\n",
    "        face2 = detect_face(new_frame)\n",
    "\n",
    "        try:\n",
    "          face1 = cv2.resize(face1, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          face2 = cv2.resize(face2, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          optical_flow = compute_optical_flow(face1, face2, number,path,True,Set)\n",
    "        except Exception as e:\n",
    "          continue\n",
    "        \n",
    "        # skip 20 frames to give randomness\n",
    "        for i in range(0,20,1):\n",
    "            video.read()\n",
    "        ret, old_frame = video.read()\n",
    "        face1 = detect_face(old_frame)\n",
    "        number +=1\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell only one time, otherwise the program will extract the frames again. We compute and save the optical flow for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoTrainAlteredPaths = list(paths.list_files(TRAIN))\n",
    "for i in videoTrainAlteredPaths:\n",
    "  frames(i,'training')\n",
    "videoValAlteredPaths = list(paths.list_files(VALIDATION))\n",
    "for i in videoValAlteredPaths:\n",
    "  frames(i,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DATA\n",
    "We then define some transformations and we use ImageFolder to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = transforms.Compose([\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "trainDataset=ImageFolder('/content/optical_flow/training',transform=tran)\n",
    "valDataset=ImageFolder('/content/optical_flow/validation',transform=tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define batch size and the device. Subsequently, we use DataLoader, which represents a Python iterable over our datasets. Our classes (altered and original) are inside class_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader=DataLoader(trainDataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader=DataLoader(valDataset,batch_size=BATCH_SIZE)\n",
    "class_name = trainDataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT A BATCH\n",
    "If you want to visualize a batch of data, you can run this function. This works also at test time, plotting the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs,labels,predictions=None):\n",
    "  total = 0\n",
    "  good = 0\n",
    "  fig = plt.figure(figsize=(21,12))\n",
    "\n",
    "  # iterate over the batch, we plot one image at a time with its label\n",
    "  for i,img in enumerate(imgs):\n",
    "    total +=1\n",
    "    fig.add_subplot(4,8, i+1)\n",
    "    label=labels[i] # actual label of the image\n",
    "    \n",
    "    # this code runs only when we have a prediction for our batch\n",
    "    if predictions==None:\n",
    "      title=\"Label: {0}\".format(class_name[label])\n",
    "    else:\n",
    "      prediction=predictions[i] # prediction done by our model\n",
    "      title=\"prediction: {0}\\nlabel:{1}\".format(class_name[prediction],class_name[label])\n",
    "      if class_name[label] == class_name[prediction]:\n",
    "        good +=1\n",
    "    \n",
    "    plt.title(title)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1) # clip values outside the interval \n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "  return [total,good] # this is usefull to predict the final accuracy\n",
    "  \n",
    "plt.show()\n",
    "\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "show(inputs,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN FUNCTION\n",
    "This is the function which is responsible to train the model. It takes in input the model which is -in our case- a pretrained model, the loss, the dataloader, the optimizer and the number of the current epoch.\n",
    "\n",
    "We store the losses and the accuracies inside a dictionary.\n",
    "\n",
    "We set the number of epochs at 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'train':[], 'val':[]}\n",
    "accuracies = {'train':[], 'val':[]}\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loss_fn,dataloader,optimizer,epoch):\n",
    "  print(f'Epoch {epoch}/{epochs}')\n",
    "\n",
    "  model.train() # model in train mode\n",
    "  \n",
    "  total_loss=0    \n",
    "  correct=0\n",
    "  total=0\n",
    "\n",
    "  for data in tqdm(dataloader):\n",
    "    \n",
    "    inputs,labels=data[0].to(device),data[1].to(device) # takes inputs and classes from the train dataset\n",
    "    \n",
    "    outputs=model(inputs) # prediction of the model\n",
    "    \n",
    "    loss=loss_fn(outputs,labels) # loss function\n",
    "    \n",
    "    optimizer.zero_grad() # gradient to zero\n",
    "    loss.backward() # backward passes\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    ret, predicted = outputs.max(1) # prediction\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "  loss=total_loss/len(dataloader)\n",
    "  accuracy=100.*correct/total\n",
    "  \n",
    "  accuracies['train'].append(accuracy)\n",
    "  losses['train'].append(loss)\n",
    "  print('Train Loss: %.3f | Accuracy: %.3f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION FUNCTION\n",
    "The val_model function is similar to the train_model function, but we do not update the weigths. At the beginning of the function we set the model to evalution mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,loss_fn,dataloader,epoch):\n",
    "  model.eval()\n",
    "\n",
    "  total_loss=0\n",
    "  correct=0\n",
    "  total=0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in tqdm(dataloader):\n",
    "      images,labels=data[0].to(device),data[1].to(device)\n",
    "      \n",
    "      outputs=model(images)\n",
    "\n",
    "      loss= loss_fn(outputs,labels)\n",
    "      total_loss+=loss.item()\n",
    "      \n",
    "      ret, predicted = outputs.max(1)\n",
    "      total += labels.size(0)\n",
    "      correct += predicted.eq(labels).sum().item()\n",
    "  \n",
    "  loss=total_loss/len(dataloader)\n",
    "  accuracy=100.*correct/total\n",
    "\n",
    "  losses['val'].append(loss)\n",
    "  accuracies['val'].append(accuracy)\n",
    "\n",
    "  print('Test Loss: %.3f | Accuracy: %.3f'%(loss,accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP THE MODEL\n",
    "Below, we setup the model. We load a pretrained model and we reset the final fully connected layer. Here, we are just finetuning the convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False #freeze part of the model do train the rest\n",
    "\n",
    "# change only the last FC layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# check the parameters to update\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "# optimizer\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL\n",
    "We train and evaluate the model for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,epochs+1): \n",
    "  train(model,loss_fn,train_loader,optimizer_ft,epoch)\n",
    "  validation(model,loss_fn,val_loader,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPHS\n",
    "We can plot training and validation loss/accuracy for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(accuracies['train'], label='Training Accuracy')\n",
    "plt.plot(accuracies['val'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(losses['train'], label='Training Loss')\n",
    "plt.plot(losses['val'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to show the prediction of our model, to check the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,images,actual_label):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    inputs = images.to(device)\n",
    "    outputs = model(inputs)\n",
    "    ret, preds = torch.max(outputs, 1)\n",
    "    pair=show(images,actual_label,preds.cpu())\n",
    "    return pair\n",
    "\n",
    "images,classes=next(iter(val_loader))\n",
    "total=0\n",
    "good=0\n",
    "for i in iter(val_loader):\n",
    "  pair = test(model,i[0],classes)\n",
    "  total +=pair[0]\n",
    "  good += pair[1]\n",
    "\n",
    "print(\"Accuracy: \"+ str((good/total)*100)+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER'S FUNCTIONS\n",
    "We define compute_optical_flow() and extract_frames() functions for a user that wants to upload a video, compute its optical flow and save it in a specific directory.\n",
    "\n",
    "Unlike the previous functions for training and validation data, these functions extract all the frames of the given video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d stefanoservillo/testtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip testtt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"finalVideo/video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames(path):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    ret, old_frame = video.read()\n",
    "    if not ret:\n",
    "        print('No frames')\n",
    "        return\n",
    "    face1 = detect_face(old_frame)\n",
    "    path = path.split('/')[3][0:-4]\n",
    "\n",
    "    number = 0\n",
    "    while(video.isOpened()):\n",
    "        ret, new_frame = video.read()\n",
    "        if not ret:\n",
    "            print(\"The video is finished\")\n",
    "            break\n",
    "        face2 = detect_face(new_frame)\n",
    "\n",
    "        try:\n",
    "          face1 = cv2.resize(face1, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          face2 = cv2.resize(face2, (300, 300), interpolation = cv2.INTER_AREA)\n",
    "          optical_flow = compute_optical_flow(face1, face2, number,path,False)\n",
    "        except Exception as e:\n",
    "          continue\n",
    "\n",
    "        old_frame = new_frame\n",
    "        face1 = face2\n",
    "        number +=1\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames('/content/prova/089_065.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs,predictions):\n",
    "  fig = plt.figure(figsize=(21,12))\n",
    "\n",
    "  total = 0\n",
    "  fake = 0\n",
    "  for i,img in enumerate(imgs):\n",
    "    total +=1\n",
    "    fig.add_subplot(4,8, i+1)\n",
    "    \n",
    "    prediction=predictions[i]\n",
    "    title=\"prediction: {0}\".format(class_name[prediction]) # take the classes of the prediction\n",
    "    if class_name[prediction] == 'altered':\n",
    "      fake +=1\n",
    "\n",
    "    plt.title(title)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "  return [total,fake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=ImageFolder('/content/finalVideo',transform=tran) # load the data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader=DataLoader(Dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(model,images):\n",
    "  model.eval()\n",
    "  with torch.no_grad(): #torch.no_grad() is used to reduce memory consumptions disabled gradient calculation\n",
    "    inputs = images.to(device) # add the inputs to the model\n",
    "    outputs = model(inputs) # obtained the prediction of the model\n",
    "    ret, preds = torch.max(outputs, 1) # returns max output with it's position\n",
    "    pair =show(images,preds.cpu()) # show the results\n",
    "    return pair\n",
    "\n",
    "total=0\n",
    "fake=0\n",
    "for i in iter(data_loader):\n",
    "  pair = predict_images(model,i[0])\n",
    "  total +=pair[0]\n",
    "  fake += pair[1]\n",
    "print(\"Video fake al \"+ str((fake/total)*100)+ '%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d2d07ecf2d88860f2c138e56ba9f113ddbe5b2c99bcbdd2c585f89a3a040e56"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
