{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwwal5V_GA1z"
      },
      "source": [
        "# **DEEPFAKE DETECTION THROUGH OPTICAL FLOW**\n",
        "\n",
        "In this notebook you will find all the required steps to build up, train and test a model which detects DeepFake videos through a combination of frames and optical flow.\n",
        "\n",
        "To run this notebook you have to add your **Kaggle Username** and **Token** in order to download the dataset from Kaggle. You can generate your API Token from your profile page.\n",
        "\n",
        "First we import the needed packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdsID6UBLZu7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "from imutils import paths\n",
        "import shutil\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm #to show progressive meter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTPo5r_CHvbB"
      },
      "source": [
        "-------------------------------------------------------------------\n",
        "# **THE DATASET**\n",
        "\n",
        "Here, we download the dataset. We chose to use **FaceForensics++** to train and test the model. \n",
        "\n",
        "This is a forensics dataset consisting of **1000 original video** sequences that have been manipulated with four automated face manipulation methods: *Deepfakes*, *Face2Face*, *FaceSwap* and *NeuralTextures*.\n",
        "\n",
        "The videos included in this subset have a compression rate factor of 23. If you want to work with the full dataset, you have to make a request through a google form to the creators. For our purpose, this one will be enough.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cxHriywn-RM"
      },
      "outputs": [],
      "source": [
        "# insert your kaggle API Token here\n",
        "os.environ['KAGGLE_USERNAME'] = \"[insert your kaggle username]\"\n",
        "os.environ['KAGGLE_KEY'] = \"[insert your kaggle API Token]\"\n",
        "\n",
        "# download the dataset\n",
        "!kaggle datasets download -d sorokin/faceforensics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22X-DBCyJQOy"
      },
      "source": [
        "After the download, we unzip all the content inside the new directory '*data*'. The process will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E5nYag-n8Hn"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/data\n",
        "\n",
        "# unzip the dataset in the new directory\n",
        "!unzip faceforensics.zip -d /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kbfgDerJymZ"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "# **HANDLE THE DATA**\n",
        "\n",
        "Before defining the model, we must handle and pre-process the dataset. \n",
        "\n",
        "Here, we specify: \n",
        "\n",
        "1.   **The current path**;\n",
        "2.   **the new paths for training, validation and test**;\n",
        "3.   **the percentage of videos we want for training, validation and test**. \n",
        "\n",
        "We chose a separation of 70% for training, 20% for validation and 10% for test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BCr6B4DL38Y"
      },
      "outputs": [],
      "source": [
        "# specify the path to the dataset\n",
        "DATASET_PATH = \"/content/data\"\n",
        "\n",
        "# specify the paths to our training, validation and test data\n",
        "TRAIN = \"train\"\n",
        "VALIDATION = \"val\"\n",
        "TEST = \"test\"\n",
        "\n",
        "# split the dataset into training, validation and test data\n",
        "TRAIN_SPLIT = 0.7\n",
        "VAL_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHqy7OqZL15V"
      },
      "source": [
        "Below, we have the '*copy_videos()*' function, which takes as input a list (containing the videos' paths) and a destination folder for our videos.\n",
        "\n",
        "In other words, we take the videos and we split them into training, validation and test videos. Then, we save them in the proper directories. \n",
        "\n",
        "We've decided to take only the DeepFake videos and the original videos. In total, **960 videos** were stored (480 DeepFakes and 480 originals)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuuZONZrMCOC"
      },
      "outputs": [],
      "source": [
        "def copy_videos(videoPaths, folder, Set):\n",
        "\n",
        "  # create the proper directory if it does not exist ('train', 'val', 'test')\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "  \n",
        "  number = 0\n",
        "  max_videos = 0\n",
        "  \n",
        "  # set a max number of videos, we take 500 videos\n",
        "  # 360 training videos for each type (altered and original)\n",
        "  if folder=='train': \n",
        "    max_videos = 960*TRAIN_SPLIT/2\n",
        "  # 60 validation videos for each type (altered and original)\n",
        "  elif folder=='val':\n",
        "    max_videos = 960*VAL_SPLIT/2\n",
        "  # 60 test videos for each type (altered and original)\n",
        "  else:\n",
        "    max_videos = 960*TEST_SPLIT/2\n",
        "  \n",
        "  while(number<max_videos):\n",
        "\n",
        "    path = videoPaths[number]\n",
        "    \n",
        "    # grab image name and its label from the path and create\n",
        "\t\t# a placeholder corresponding to the separate label folder\n",
        "    videoName = path.split(os.path.sep)[-1]\n",
        "    labelFolder = os.path.join(folder, Set)\n",
        "\t\t\n",
        "    # check to see if the label folder exists and if not create it\n",
        "    if not os.path.exists(labelFolder):\n",
        "      os.makedirs(labelFolder)\n",
        "\t\t\n",
        "    # construct the destination image path and copy the current\n",
        "\t\t# image to it\n",
        "    destination = os.path.join(labelFolder, videoName)\n",
        "    shutil.copy(path, destination)\n",
        "  \n",
        "    number+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHCQY9c1n48O"
      },
      "outputs": [],
      "source": [
        "# load all altered videos paths and randomly shuffle them\n",
        "print(\"[INFO] loading video paths...\")\n",
        "videoAlteredPaths = list(paths.list_files(DATASET_PATH+'/manipulated_sequences/Deepfakes/c23/videos'))\n",
        "np.random.shuffle(videoAlteredPaths)\n",
        "\n",
        "# generate altered training, validation and test paths\n",
        "valAlteredPathsLen = int(len(videoAlteredPaths) * VAL_SPLIT)\n",
        "trainAlteredPathsLen = int(len(videoAlteredPaths) * TRAIN_SPLIT)\n",
        "trainPaths = videoAlteredPaths[:trainAlteredPathsLen]\n",
        "valPaths = videoAlteredPaths[trainAlteredPathsLen:trainAlteredPathsLen+valAlteredPathsLen]\n",
        "testPaths = videoAlteredPaths[trainAlteredPathsLen+valAlteredPathsLen:]\n",
        "\n",
        "# copy the altered training, validation and test videos to their respective\n",
        "# directories\n",
        "print(\"[INFO] copying training , validation and test altered videos...\")\n",
        "copy_videos(trainPaths, TRAIN, \"altered\")\n",
        "copy_videos(valPaths, VALIDATION, \"altered\")\n",
        "copy_videos(testPaths, TEST, \"altered\")\n",
        "\n",
        "# load all the original videos paths and randomly shuffle them\n",
        "print(\"[INFO] loading video paths...\")\n",
        "videoOriginalPaths = list(paths.list_files(DATASET_PATH+'/original_sequences/youtube/c23/videos'))\n",
        "np.random.shuffle(videoOriginalPaths)\n",
        "\n",
        "# generate original training, validation and test paths\n",
        "valOriginalPathsLen = int(len(videoOriginalPaths) * VAL_SPLIT)\n",
        "trainOriginalPathsLen = int(len(videoOriginalPaths) * TRAIN_SPLIT)\n",
        "trainPaths = videoOriginalPaths[:trainOriginalPathsLen]\n",
        "valPaths = videoOriginalPaths[trainOriginalPathsLen:trainOriginalPathsLen+valOriginalPathsLen]\n",
        "testPaths = videoOriginalPaths[trainOriginalPathsLen+valOriginalPathsLen:]\n",
        "\n",
        "# copy the original training, validation and test videos to their respective\n",
        "# directories\n",
        "print(\"[INFO] copying training , validation and test original videos...\")\n",
        "copy_videos(trainPaths, TRAIN, \"original\")\n",
        "copy_videos(valPaths, VALIDATION, \"original\")\n",
        "copy_videos(testPaths, TEST, \"original\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxtQxKsTIYsC"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# **OPTICAL FLOW**\n",
        "\n",
        "Here, we present the main pre-processing part, where we take each video, we extract the frames and we compute the optical flow.\n",
        "\n",
        "Since we had a huge number of videos and we didn't use any Deep Learning method, the computation of the optical flow (for each pair of frames) would have been to slow. So, before estimating the optical flow, we've decided to extract the face from each frame and then we've computed the dense optical flow between subsequent frames.\n",
        "\n",
        "At the end, we save a **fusion** between the RGB frames and the optical flow. \n",
        "\n",
        "We begin by creating the directories where we store the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYaQLNniSD9g"
      },
      "outputs": [],
      "source": [
        "opticalPath = \"/content/optical_fusion/\"\n",
        "\n",
        "if not os.path.exists(opticalPath):\n",
        "  os.makedirs(opticalPath)\n",
        "  os.makedirs(opticalPath+\"training/original\")\n",
        "  os.makedirs(opticalPath+\"training/altered\")\n",
        "  os.makedirs(opticalPath+\"validation/original\")\n",
        "  os.makedirs(opticalPath+\"validation/altered\")\n",
        "  os.makedirs(opticalPath+\"test/original\")\n",
        "  os.makedirs(opticalPath+\"test/altered\")\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k70mPej2MAJP"
      },
      "source": [
        "This function is meant to take as input the **BGR frame** and the **face cascade classifier** which is, in this case, the haar cascade frontalface classifier. \n",
        "\n",
        "We've chosen this one after several trials, optimizing the parameters, because it's the one that better detected the faces in our frames. The classifier is defined outside the function to avoid recalling it for each frame. This saves us a lot of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egG62tEVSM0q"
      },
      "outputs": [],
      "source": [
        "# define the classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
        "\n",
        "def detect_face(frame_bgr, face_cascade):\n",
        "    \n",
        "    # convert the frame to gray, the classifier works with gray images\n",
        "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)  \n",
        "\n",
        "    if (len(faces) == 0):\n",
        "        return None\n",
        "    \n",
        "    for (x, y, w, h) in faces:\n",
        "        continue\n",
        "    \n",
        "    # return the BGR face\n",
        "    return frame_bgr[y - 40 : y + w + 40, x - 40 : x + h + 40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bankPY7AONH6"
      },
      "source": [
        "The '*bgr_fusion()*' function takes as input the BGR frames and the optical flow (which was converted in a HSV representation and then in a BGR image).\n",
        "First, it adds (**blends**) the two BGR frames with equal weights, then it blends the result with the optical flow, but this time with different weights. At the end, it returns this fusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_1IEkBt4I1d"
      },
      "outputs": [],
      "source": [
        "def bgr_fusion(face1, face2, optical_flow):\n",
        "  \n",
        "  # average between the two frames\n",
        "  facesWeighted = cv2.addWeighted(face1, 0.5, face2, 0.5, 0)\n",
        "  \n",
        "  # weighted average between the two frames and the optical flow\n",
        "  optical_fusion = cv2.addWeighted(facesWeighted, 0.7, optical_flow, 0.3, 0)\n",
        "  \n",
        "  return optical_fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-Kow7BYpTD"
      },
      "source": [
        "The '*compute_optical_flow()*' function takes in input the **two frames** (the crop of the faces), the type and the name (**path**) of the video, the optical flow **number** and the **set** (training, validation or test). For training and validation we compute 10 optical flow per video, so the number can be 0, 1 or 2. For test we compute the optical flow almost for all the frames.\n",
        "\n",
        "The optical flow is computed using a the **Farneback** method, which is a dense method. First, we creates an array filled with zero with the same dimensions of the BGR frame. Then, we compute the optical flow, we extract the magnitude and angle of the 2D vectors and finally, we set image hue and value according to the optical flow direction and magnitude. Then we convert HSV to RGB (BGR) color representation. \n",
        "\n",
        "With this optical flow and the two frames we call the '*bgr_fusion()*' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmNEkGTFSOvK"
      },
      "outputs": [],
      "source": [
        "def compute_optical_flow(face1_bgr, face2_bgr, number, path, Set=None):\n",
        "    \n",
        "    # convert grayscale frames into bgr frames\n",
        "    face1_gray = cv2.cvtColor(face1_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    face2_gray = cv2.cvtColor(face2_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Creates an array filled with zero \n",
        "    # with the same dimensions of the frame\n",
        "    hsv = np.zeros_like(face1_bgr)\n",
        "    hsv[..., 1] = 255\n",
        "\n",
        "    # Compute the optical flow\n",
        "    flow = cv2.calcOpticalFlowFarneback(face1_gray, face2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    \n",
        "    # Magnitude and angle of the 2D vectors\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Sets image hue and value according to the optical flow direction\n",
        "    # and magnitude, then converts HSV to RGB (BGR) color representation\n",
        "    hsv[..., 0] = ang*180/np.pi/2\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    optical_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # fuse the frames with the optical flow and convert to rgb\n",
        "    fused_bgr = bgr_fusion(face1_bgr, face2_bgr, optical_bgr)\n",
        "    fused_rgb = cv2.cvtColor(fused_bgr, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # save the optical flow (fused with the frames) as RGB  \n",
        "    Image.fromarray(fused_rgb).save(opticalPath+'{}/{}-{}.jpg'.format(Set,path,number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTV-LE-tUll"
      },
      "source": [
        "The '*frames()*' function takes in input the video path and the set of the video. For training and validation data, we compute 10 optical flows, while for test data we take 30 optical flows (avoiding redundancy by skipping frames).\n",
        "We've defined some parameters in order to obtain the same number of optical flows for videos of different lenghts. \n",
        "\n",
        "First, we capture the video with VideoCapture(). Then, we iterate several times (according to the set) the following process:\n",
        "\n",
        "1.   **Read two frames**;\n",
        "2.   **detect the faces**;\n",
        "3.   **resize the faces to 224x224**;\n",
        "4.   **compute the optical flow**;\n",
        "5.   **skip some frames to avoid redundacy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrz9aL9fSQoI"
      },
      "outputs": [],
      "source": [
        "def frames(path, Set):\n",
        "    \n",
        "    # capture the video\n",
        "    video = cv2.VideoCapture(path)\n",
        "    \n",
        "    # type (original or altered) and name of the video\n",
        "    path = path.split('/')[1]+'/'+ path.split('/')[2][0:-4]\n",
        "    \n",
        "    number = 0\n",
        "    \n",
        "    # set some parameters according to the set and the video lenght:\n",
        "    # number of optical flows computed and number of frames skipped\n",
        "    video_lenght = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    long = video_lenght > 200\n",
        "    \n",
        "    if Set == 'training' or Set == 'validation':\n",
        "      max = 10\n",
        "      if long:\n",
        "        skip = 16\n",
        "      else:\n",
        "        skip = 8\n",
        "  \n",
        "    elif Set == 'test':\n",
        "      max = 30\n",
        "      if long:\n",
        "        skip = 4\n",
        "      else:\n",
        "        skip = 2\n",
        "      \n",
        "    # iterate over the video to compute the optical flow\n",
        "    while(number<max):\n",
        "          \n",
        "      # capture the first frame\n",
        "      ret, old_frame = video.read()\n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      # capture the second frame\n",
        "      ret, new_frame = video.read()\n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      # detect the faces \n",
        "      face1 = detect_face(old_frame, face_cascade)\n",
        "      face2 = detect_face(new_frame, face_cascade)\n",
        "\n",
        "      try:\n",
        "        # resize the faces\n",
        "        face1 = cv2.resize(face1, (224, 224), interpolation = cv2.INTER_AREA)\n",
        "        face2 = cv2.resize(face2, (224, 224), interpolation = cv2.INTER_AREA)\n",
        "            \n",
        "        # compute the optical flow\n",
        "        compute_optical_flow(face1, face2, number, path, Set)\n",
        "        number +=1\n",
        "\n",
        "      except Exception as e:\n",
        "        continue\n",
        "          \n",
        "      # skip 20 frames to give randomness\n",
        "      for i in range(0, skip, 1):\n",
        "        video.read()\n",
        "\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeJbkNV7wAr_"
      },
      "source": [
        "Below, we recall the '*frames()*' function for each element of a list containing the video paths. We do that for the three sets.\n",
        "\n",
        "This process will take a while (**1h30m/2h**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA78o8z6n1yU"
      },
      "outputs": [],
      "source": [
        "# take the training video paths and extract the frames \n",
        "videoTrainPaths = list(paths.list_files(TRAIN))\n",
        "print(\"Extracting training frames:\")\n",
        "for i in tqdm(videoTrainPaths):\n",
        "  frames(i,'training')\n",
        "\n",
        "# take the validation video paths and extract the frames\n",
        "videoValPaths = list(paths.list_files(VALIDATION))\n",
        "print(\"Extracting validation frames:\")\n",
        "for i in tqdm(videoValPaths):\n",
        "  frames(i,'validation')\n",
        "\n",
        "# take the test video paths and extract the frames\n",
        "videoTestPaths = list(paths.list_files(TEST))\n",
        "print(\"Extracting test frames:\")\n",
        "for i in tqdm(videoTestPaths):\n",
        "  frames(i,'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEKt3cPmy3z-"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# **LOAD THE DATASET**\n",
        "\n",
        "Now, it's time to load the dataset. We **transform** the image in a PyTorch tensor and we **normalize** the data. The input images have a 224x224 resolution and they are RGB images (a fusion between the optical flow and the frames). It is important to give as input data that have similar properties to those on which the model has been trained on.\n",
        "\n",
        "We'll also be doing data augmentation, trying to improve the performance of the model by forcing it to learn about images at different angles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XwaaEPfo1RsH"
      },
      "outputs": [],
      "source": [
        "# mean and standard deviation\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "# apply some transformations\n",
        "train_tran = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "val_tran = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kuFf6jG6gQc"
      },
      "source": [
        "\n",
        "Then, we load the training and validation set with ImageFolder and DataLoader. The **batch size** is 128, the number of **epochs** is 50. \n",
        "\n",
        "Subsequently, we store the **dataset classes**, the training and validation datasets' **lenght**, and the **device**; if the GPU is available, the training, validation and test phases will be performed on it. We also create two dictionaries to store the losses and the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HaJW2OuSSldS"
      },
      "outputs": [],
      "source": [
        "# define the batch size and the number of epochs\n",
        "BATCH_SIZE = 128\n",
        "epochs = 50\n",
        "\n",
        "# load the data using ImageFolder and DataLoader\n",
        "trainDataset=ImageFolder('/content/optical_fusion/training',transform=train_tran)\n",
        "valDataset=ImageFolder('/content/optical_fusion/validation',transform=val_tran)\n",
        "testDataset=ImageFolder('/content/optical_fusion/test',transform=val_tran)\n",
        "\n",
        "train_loader=DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader=DataLoader(valDataset,batch_size=256, shuffle=True)\n",
        "test_loader=DataLoader(testDataset,batch_size=BATCH_SIZE)\n",
        "\n",
        "# data classes\n",
        "class_name = trainDataset.classes\n",
        "\n",
        "# training, validation and test datasets' lenght\n",
        "train_size = len(train_loader) \n",
        "val_size = len(val_loader)\n",
        "test_size = len(test_loader)\n",
        "\n",
        "# dictionaries to store the losses and the accuracies\n",
        "losses = {'train':[], 'val':[]}\n",
        "accuracies = {'train':[], 'val':[]}\n",
        "\n",
        "# define the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqJSzxA7cWwS"
      },
      "source": [
        "--------------------------------------------------\n",
        "# **TRAINING AND VALIDATION FUNCTIONS**\n",
        "\n",
        "At this point, we have to use our data to **train** and **validate** a model. \n",
        "For every epoch, we train and validate the model to keep track of how it improves. In order to do this, we wrote the training and valitation functions.\n",
        "\n",
        "In the *train()* function, we feed the inputs and the labels (organized in batches) to our model, which gives back the predictions. Then, we compute the loss, we back propagate and we update the weights. \n",
        "\n",
        "The accuracy is obtained using softmax: we apply it on each frame, we get a prediction and we average this prediction along the batch. At the end we average the prediction on the whole set.\n",
        "\n",
        "The epoch losses and accuracies are then saved in dictionaries and they are printed in the console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZCUpwi7uLr0V"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, optimizer, sched, epoch):\n",
        "  print(f'Epoch {epoch}/{epochs}')\n",
        "\n",
        "  # model in train mode\n",
        "  model.train() \n",
        "  \n",
        "  # total accuracy and total loss\n",
        "  total_accuracy = 0   \n",
        "  total_loss = 0 \n",
        "\n",
        "  # iterate for each batch of data\n",
        "  for data in tqdm(train_loader):\n",
        "\n",
        "    accuracy = 0\n",
        "    \n",
        "    # take the input and send it to the GPU\n",
        "    inputs, labels = data[0].to(device), data[1].to(device) # takes inputs and classes from the train dataset\n",
        "    \n",
        "    # get the predictions\n",
        "    outputs = model(inputs) \n",
        "\n",
        "    # apply softmax on the output and get a tensor of probabilities\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)*100\n",
        "    _, indices = torch.sort(outputs, descending = True)\n",
        "\n",
        "    for i in range(len(probabilities)):\n",
        "      for j in indices[i][:2]:\n",
        "        if j == labels[i].item():\n",
        "          accuracy += probabilities[i][j].item()\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    # zero grad\n",
        "    optimizer.zero_grad()\n",
        "    # back propagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # update loss and accuracy\n",
        "    accuracy = round(accuracy/len(labels), 3)\n",
        "    total_accuracy += accuracy\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  # scheduler step\n",
        "  sched.step()\n",
        "\n",
        "  #compute epoch loss\n",
        "  epoch_accuracy = round(total_accuracy/train_size, 3)\n",
        "  epoch_loss = round(total_loss/train_size, 3)\n",
        "  \n",
        "  # save epoch losses and accuracies and print them\n",
        "  accuracies['train'].append(epoch_accuracy)\n",
        "  losses['train'].append(epoch_loss)\n",
        "  print('Train Loss: %.3f | Accuracy: %.3f'%(epoch_loss, epoch_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lC02n5mdLum5"
      },
      "outputs": [],
      "source": [
        "def validation(model, loss_fn, epoch):\n",
        "  # model in train mode\n",
        "  model.eval() \n",
        "\n",
        "  # total accuracy and total loss\n",
        "  total_loss=0\n",
        "  total_accuracy = 0 \n",
        "\n",
        "  with torch.no_grad():\n",
        "    # iterate for each batch of data\n",
        "    for data in tqdm(val_loader):\n",
        "      \n",
        "      accuracy = 0\n",
        "      \n",
        "      # take the input and send it to the GPU\n",
        "      images, labels=data[0].to(device), data[1].to(device)\n",
        "      \n",
        "      # get the predictions\n",
        "      outputs=model(images)\n",
        "\n",
        "      # apply softmax on the output and get a tensor of probabilities\n",
        "      probabilities = torch.nn.functional.softmax(outputs, dim=1)*100\n",
        "      _, indices = torch.sort(outputs, descending = True)\n",
        "\n",
        "      for i in range(len(probabilities)):\n",
        "        for j in indices[i][:2]:\n",
        "          if j == labels[i].item():\n",
        "            accuracy += probabilities[i][j].item()\n",
        "\n",
        "      # compute the loss\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      # update loss and accuracy\n",
        "      total_loss += loss.item()\n",
        "      accuracy = round(accuracy/len(labels), 3)\n",
        "      total_accuracy += accuracy\n",
        "\n",
        "  # compute epoch loss\n",
        "  epoch_accuracy = round(total_accuracy/val_size, 3)  \n",
        "  epoch_loss = round(total_loss/val_size, 3)\n",
        "  \n",
        "  # save epoch losses and accuracies and print them\n",
        "  losses['val'].append(epoch_loss)\n",
        "  accuracies['val'].append(epoch_accuracy)\n",
        "  print('Validation Loss: %.3f | Accuracy: %.3f'%(epoch_loss, epoch_accuracy)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5c793s2nqlG"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# **THE MODEL**\n",
        "The model we used is a pretrained **ResNet18**. To avoid overfitting, we froze the first 2/3 layers, keeping the weights untouched. The last fully connected layer was replaced by a new fully connected one with 2 output neurons. \n",
        "\n",
        "We trained this model for 50 epochs using a **cross entropy loss**, the **stochastic gradient descent** as optimizer and a **step scheduler**.\n",
        "\n",
        "The scheduler is meant to reduce the learning rate every 'step_size' epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R1cef4t-LxAj"
      },
      "outputs": [],
      "source": [
        "# ResNet18 with pretrained weights on ImageNet\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# freeze some parameters\n",
        "ct = 0\n",
        "for child in model.children():\n",
        "  ct += 1\n",
        "  if ct < 3:\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "# change the last FC layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "# send the model to the GPU (or to CPU if you don't have it)\n",
        "model = model.to(device)\n",
        "\n",
        "# define the loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD Optimizer\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "#StepLr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=6, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvR3YInHnyUU"
      },
      "outputs": [],
      "source": [
        "# train and validate for 50 epochs\n",
        "for epoch in range(1,epochs+1): \n",
        "  train(model,loss_fn, optimizer_ft, scheduler, epoch)\n",
        "  validation(model, loss_fn, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf6YpCoSrivr"
      },
      "source": [
        "-------------------------------------------------------------------------------\n",
        "# **Plot the results**\n",
        "\n",
        "Below, we can see the results. Our model reaches an accuracy of 95% on the training set, while for the validation set, it has 80-85% accuracy.\n",
        "We still have a little of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMFNPklDnb9q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11, 11))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracies['train'], label='Training Accuracy')\n",
        "plt.plot(accuracies['val'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(losses['train'], label='Training Loss')\n",
        "plt.plot(losses['val'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMWQC_qo0UWe"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# **TEST THE MODEL ON THE TEST SET**\n",
        "Here, we test our final model with the test set. Since it would have been tricky to test the videos separately, we've decided to average the softmax for all the frames. This is the same thing of averaging the prediction for each video since the number of frames taken per video is the same.\n",
        "\n",
        "We've also added an accuracy indicator for original and altered set to highlight how the model performs on these two types of videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv7utEo2nffB"
      },
      "outputs": [],
      "source": [
        "# set the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  fake_accuracy =0\n",
        "  fake_video = 0\n",
        "  original_accuracy = 0\n",
        "  original_video = 0\n",
        "  video_accuracy = 0\n",
        "  \n",
        "  # iterate for each batch\n",
        "  for data in test_loader:\n",
        "    \n",
        "    # take the input and send it to the GPU\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    # get the predictions from the model \n",
        "    outputs = model(images)\n",
        "\n",
        "    # get the probabilities and compute the accuracy\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)*100\n",
        "    _, indices = torch.sort(outputs, descending = True)\n",
        "    \n",
        "    for i in range(len(probabilities)):\n",
        "      for j in indices[i][:2]:\n",
        "        if j==labels[i].item():\n",
        "          video_accuracy += probabilities[i][j].item()\n",
        "          if labels[i].item() == 0:\n",
        "            fake_accuracy += probabilities[i][j].item()\n",
        "            fake_video +=1\n",
        "          else:\n",
        "            original_accuracy += probabilities[i][j].item()\n",
        "            original_video +=1\n",
        "  \n",
        "  # print the results\n",
        "  fake_accuracy = round(fake_accuracy/fake_video,2)\n",
        "  print(\"Total Accuracy for fake videos is: {}\".format(fake_accuracy))\n",
        "  print()\n",
        "  original_accuracy = round(original_accuracy/original_video,2)\n",
        "  print(\"Total Accuracy for original videos is: {}\".format(original_accuracy))\n",
        "  print()\n",
        "  total_accuracy = round(video_accuracy/(fake_video+original_video),2)\n",
        "  print(\"Total Accuracy is: {}\".format(total_accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepFake_Detection.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
